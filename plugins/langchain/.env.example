# ===========================================
# LangChain Plugin Configuration
# ===========================================

# Plugin Settings
LANGCHAIN_PLUGIN_ENABLED=true
LANGCHAIN_PLUGIN_DEBUG=false

# File Logging (core environment variable)
# Logs to logger/ai/ when enabled
LOG_ENABLED=false

# ===========================================
# Ollama Configuration (Local)
# ===========================================
# Used by: single-agent
# No API key required - runs locally

# Ollama server URL
LANGCHAIN_OLLAMA_BASE_URL=http://localhost:11434

# Model to use (run `ollama list` to see available models)
LANGCHAIN_OLLAMA_MODEL=llama3.2:3b

# ===========================================
# OpenAI-compatible Configuration (LM Studio)
# ===========================================
# Used by: orchestrator, task-assistant, customer-assistant, page-assistant
#
# LM Studio provides an OpenAI-compatible API on localhost.
# 1. Start LM Studio and load a model
# 2. Start the local server (default: http://localhost:1234/v1)

# LM Studio server URL
LANGCHAIN_OPENAI_BASE_URL=http://localhost:1234/v1

# Model name as loaded in LM Studio
LANGCHAIN_OPENAI_MODEL=your-loaded-model-name

# API Key (LM Studio doesn't require a real key, use any string)
OPENAI_API_KEY=lm-studio
