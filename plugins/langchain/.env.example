# ============================================
# LANGCHAIN PLUGIN CONFIGURATION
# ============================================
#
# Copy this file to .env and configure your LangChain settings
# This file is loaded automatically by the LangChain plugin
#
# Priority: Plugin .env > Root .env > Defaults
#
# ============================================

# ===========================
# Plugin Settings
# ===========================

# Enable/disable the LangChain plugin
LANGCHAIN_PLUGIN_ENABLED=true

# Enable debug logging for development
LANGCHAIN_PLUGIN_DEBUG=false

# Enable file logging (logs to logger/ai/)
LOG_ENABLED=false

# ===========================
# Ollama Configuration (Local)
# ===========================
# No API key required - runs locally
# Install Ollama: https://ollama.ai/download
# Pull a model: ollama pull llama3.2:3b

# Ollama server URL
LANGCHAIN_OLLAMA_BASE_URL=http://localhost:11434

# Default model (run `ollama list` to see available models)
LANGCHAIN_OLLAMA_MODEL=llama3.2:3b

# ===========================
# OpenAI Configuration
# ===========================
# Get your key from: https://platform.openai.com/api-keys

# OpenAI API Key
OPENAI_API_KEY=your-openai-key-here

# Default model for OpenAI provider
LANGCHAIN_OPENAI_MODEL=gpt-4o-mini

# OpenAI-compatible base URL (for LM Studio, local servers)
# LM Studio: http://localhost:1234/v1
# Leave empty for official OpenAI API
# LANGCHAIN_OPENAI_BASE_URL=http://localhost:1234/v1

# ===========================
# Anthropic Configuration
# ===========================
# Get your key from: https://console.anthropic.com/

# Anthropic API Key
ANTHROPIC_API_KEY=your-anthropic-key-here

# Default model for Anthropic provider
LANGCHAIN_ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# ===========================
# Feature Flags
# ===========================

# Enable graph-based orchestrator (recommended for multi-agent)
LANGCHAIN_USE_GRAPH_ORCHESTRATOR=true

# Enable verbose debug output
LANGCHAIN_DEBUG=false
